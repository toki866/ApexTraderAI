# -*- coding: utf-8 -*-
"""tools/check_stepE_reward_alignment.py を更新して --tag を追加します（old/ に自動バックアップ）。"""

from __future__ import annotations

import datetime
from pathlib import Path

NEW_CONTENT = '#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n"""\nStepE の daily log (Date/pos/ret) が「どのリターン定義」に一致しているかを監査します。\n\n目的:\n- ret が同日 (D) の値動きを使っていないか（=リーク疑い）を切り分ける\n- ret が翌日 (D->D+1) の値動きで確定しているか（=タイミング安全）を確認する\n- pos が pos(D) か pos(D-1) のどちらで掛けられているかも推定する\n\n入力:\n- output/stepE/<mode>/*.csv  (StepE daily log)\n- output/stepA/<mode>/daily/stepA_daily_features_<SYMBOL>_YYYY_MM_DD.csv  (Open/Close を含む)\n\n使い方（例）:\n  python tools/check_stepE_reward_alignment.py --symbol SOXL --mode sim --trade-cost-bps 5\n  python tools/check_stepE_reward_alignment.py --symbol SOXL --mode sim --log output/stepE/sim/stepE_daily_log_dprime_bnf_h01_SOXL.csv\n  python tools/check_stepE_reward_alignment.py --symbol SOXL --mode sim --all\n\n注意:\n- cmd.exe のバックスラッシュ例はエスケープ警告になりやすいので、この docstring では / を使っています。\n"""\n\nfrom __future__ import annotations\n\nimport argparse\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport pandas as pd\n\n\nLOG_GLOB_PATTERNS = [\n    "stepE_daily_log_*_{symbol}.csv",\n    "stepE_log_*_{symbol}.csv",\n    "*stepE*log*{symbol}*.csv",\n]\n\n\ndef _mode_candidates(output_root: Path, mode: str, symbol: str) -> List[Path]:\n    base = output_root / "stepE" / mode\n    cands: List[Path] = []\n    for pat in LOG_GLOB_PATTERNS:\n        for p in base.glob(pat.format(symbol=symbol)):\n            if p.is_file():\n                cands.append(p)\n    # unique\n    seen = set()\n    uniq = []\n    for p in cands:\n        rp = str(p).replace("\\\\", "/")\n        if rp not in seen:\n            seen.add(rp)\n            uniq.append(p)\n    return sorted(uniq, key=lambda x: x.stat().st_mtime, reverse=True)\n\n\ndef _pick_default_log(cands: List[Path]) -> Path:\n    # newest first already\n    return cands[0]\n\n\ndef _find_col(df: pd.DataFrame, keys: List[str]) -> Optional[str]:\n    cols = list(df.columns)\n    lower = {c.lower(): c for c in cols}\n    for k in keys:\n        if k.lower() in lower:\n            return lower[k.lower()]\n    # fuzzy contains\n    for k in keys:\n        kl = k.lower()\n        for c in cols:\n            if kl in c.lower():\n                return c\n    return None\n\n\ndef _parse_date_series(s: pd.Series) -> pd.Series:\n    # tolerate "YYYY-MM-DD" and "YYYY/MM/DD" and "YYYY_MM_DD"\n    ss = s.astype(str).str.strip()\n    ss = ss.str.replace("_", "-", regex=False).str.replace("/", "-", regex=False)\n    return pd.to_datetime(ss, errors="coerce").dt.normalize()\n\n\ndef _load_log(path: Path) -> pd.DataFrame:\n    df = pd.read_csv(path)\n    date_col = _find_col(df, ["Date", "date", "dt", "Day"])\n    if date_col is None:\n        raise RuntimeError(f"Date列が見つかりません: {path}")\n    df = df.copy()\n    df["__Date"] = _parse_date_series(df[date_col])\n    df = df.dropna(subset=["__Date"]).sort_values("__Date").reset_index(drop=True)\n\n    pos_col = _find_col(df, ["pos", "position", "Pos", "action_pos", "portfolio_pos"])\n    ret_col = _find_col(df, ["ret", "reward", "pnl", "return", "Ret", "r"])\n    if pos_col is None:\n        raise RuntimeError(f"pos列が見つかりません: {path}  columns={list(df.columns)}")\n    if ret_col is None:\n        raise RuntimeError(f"ret列が見つかりません: {path}  columns={list(df.columns)}")\n\n    df["__pos"] = pd.to_numeric(df[pos_col], errors="coerce")\n    df["__ret"] = pd.to_numeric(df[ret_col], errors="coerce")\n    df = df.dropna(subset=["__pos", "__ret"]).reset_index(drop=True)\n    return df\n\n\ndef _features_path(output_root: Path, mode: str, symbol: str, dt: pd.Timestamp) -> Path:\n    d = dt.strftime("%Y_%m_%d")\n    return output_root / "stepA" / mode / "daily" / f"stepA_daily_features_{symbol}_{d}.csv"\n\n\ndef _load_day_features(path: Path) -> Optional[pd.Series]:\n    if not path.exists():\n        return None\n    try:\n        df = pd.read_csv(path)\n        if len(df) == 0:\n            return None\n        return df.iloc[0]\n    except Exception:\n        return None\n\n\n@dataclass\nclass CandidateResult:\n    implied: str               # "ret/pos(D)" etc\n    return_def: str            # "cc_same" etc\n    n: int\n    mae: float\n    rmse: float\n    corr: float\n\n\ndef _safe_corr(a: pd.Series, b: pd.Series) -> float:\n    try:\n        if a.nunique() < 2 or b.nunique() < 2:\n            return float("nan")\n        return float(a.corr(b))\n    except Exception:\n        return float("nan")\n\n\ndef _metrics(a: pd.Series, b: pd.Series) -> Tuple[int, float, float, float]:\n    df = pd.concat([a, b], axis=1).dropna()\n    n = len(df)\n    if n == 0:\n        return 0, float("nan"), float("nan"), float("nan")\n    err = df.iloc[:, 0] - df.iloc[:, 1]\n    mae = float(err.abs().mean())\n    rmse = float((err * err).mean() ** 0.5)\n    corr = _safe_corr(df.iloc[:, 0], df.iloc[:, 1])\n    return n, mae, rmse, corr\n\n\ndef analyze_one(log_path: Path, output_root: Path, mode: str, symbol: str, trade_cost_bps: float) -> List[CandidateResult]:\n    L = _load_log(log_path)\n\n    # Build a unified table with Open/Close for D-1, D, D+1 where possible\n    dates = list(L["__Date"])\n    feat_cache: Dict[pd.Timestamp, Optional[pd.Series]] = {}\n    def get_feat(dt: pd.Timestamp) -> Optional[pd.Series]:\n        if dt in feat_cache:\n            return feat_cache[dt]\n        p = _features_path(output_root, mode, symbol, dt)\n        s = _load_day_features(p)\n        feat_cache[dt] = s\n        return s\n\n    opens = []\n    closes = []\n    close_prev = []\n    open_next = []\n    close_next = []\n    has = []\n\n    for dt in dates:\n        f = get_feat(dt)\n        f_prev = get_feat(dt - pd.Timedelta(days=1))\n        f_next = get_feat(dt + pd.Timedelta(days=1))\n        # We may miss prev/next due to weekends; use trading-day adjacency via available files:\n        # We\'ll also try to infer prev/next by looking at sorted available log dates, not calendar day.\n        opens.append(float(f["Open"]) if f is not None and "Open" in f else float("nan"))\n        closes.append(float(f["Close"]) if f is not None and "Close" in f else float("nan"))\n        close_prev.append(float(f_prev["Close"]) if f_prev is not None and "Close" in f_prev else float("nan"))\n        open_next.append(float(f_next["Open"]) if f_next is not None and "Open" in f_next else float("nan"))\n        close_next.append(float(f_next["Close"]) if f_next is not None and "Close" in f_next else float("nan"))\n        has.append(f is not None)\n\n    T = pd.DataFrame({\n        "Date": dates,\n        "Open": opens,\n        "Close": closes,\n        "Close_prev_cal": close_prev,\n        "Open_next_cal": open_next,\n        "Close_next_cal": close_next,\n        "pos": L["__pos"].astype(float).values,\n        "ret": L["__ret"].astype(float).values,\n    })\n\n    # Improve prev/next with *log-date adjacency* (trading days)\n    T = T.sort_values("Date").reset_index(drop=True)\n    T["Close_prev"] = T["Close"].shift(1)\n    T["Open_next"] = T["Open"].shift(-1)\n    T["Close_next"] = T["Close"].shift(-1)\n\n    # Candidate return definitions\n    eps = 1e-12\n    T["oc_same"] = T["Close"] / T["Open"] - 1.0\n    T["cc_same"] = T["Close"] / T["Close_prev"] - 1.0\n    T["oc_next"] = T["Close_next"] / T["Open_next"] - 1.0\n    T["cc_next"] = T["Close_next"] / T["Close"] - 1.0\n\n    # Trade cost model (simple): cost = bps/10000 * |pos - prev_pos|\n    bps = float(trade_cost_bps)\n    T["pos_prev"] = T["pos"].shift(1)\n    T["dpos"] = (T["pos"] - T["pos_prev"]).abs()\n    T["cost"] = (bps / 10000.0) * T["dpos"].fillna(0.0)\n\n    # Two implied-return hypotheses:\n    #   H1: ret = pos(D) * r + cost_term\n    #   H2: ret = pos(D-1) * r + cost_term\n    # We test both with and without adding back cost (unknown sign conventions).\n    def implied_series(div_by: str, add_cost: bool) -> pd.Series:\n        denom = T[div_by].copy()\n        denom = denom.where(denom.abs() > eps)\n        numer = T["ret"] + (T["cost"] if add_cost else 0.0)\n        return numer / denom\n\n    implied_defs = [\n        ("ret/pos(D) (no_cost_adj)", implied_series("pos", add_cost=False)),\n        ("ret/pos(D) (add_cost)", implied_series("pos", add_cost=True)),\n        ("ret/pos(D-1) (no_cost_adj)", implied_series("pos_prev", add_cost=False)),\n        ("ret/pos(D-1) (add_cost)", implied_series("pos_prev", add_cost=True)),\n    ]\n    return_defs = ["oc_same", "cc_same", "oc_next", "cc_next"]\n\n    results: List[CandidateResult] = []\n    for iname, iser in implied_defs:\n        for rname in return_defs:\n            n, mae, rmse, corr = _metrics(iser, T[rname])\n            results.append(CandidateResult(implied=iname, return_def=rname, n=n, mae=mae, rmse=rmse, corr=corr))\n\n    # Sort: prefer larger n, then lower mae, then lower rmse, then higher corr\n    results.sort(key=lambda x: (-x.n, x.mae if not pd.isna(x.mae) else 1e9, x.rmse if not pd.isna(x.rmse) else 1e9, -(x.corr if not pd.isna(x.corr) else -1e9)))\n    return results\n\n\ndef main() -> int:\n    ap = argparse.ArgumentParser()\n    ap.add_argument("--symbol", required=True)\n    ap.add_argument("--mode", default="sim", choices=["sim", "live", "ops", "display"])\n    ap.add_argument("--output-root", default="output")\n    ap.add_argument("--log", default=None, help="StepE log csv path. If omitted, newest candidate is auto-selected (or use --all).")\n    ap.add_argument("--tag", default=None, help="Filter by substring in log filename (e.g., dprime_all_features_h01). Can be used with --all.")\n    ap.add_argument("--all", action="store_true", help="Analyze all matching logs under output/stepE/<mode> for this symbol.")\n    ap.add_argument("--trade-cost-bps", type=float, default=0.0)\n    args = ap.parse_args()\n\n    output_root = Path(args.output_root)\n    mode = args.mode\n    symbol = args.symbol\n\n    if args.all and args.log:\n        raise SystemExit("--all と --log は同時に指定できません。")\n\n    if args.all:\n        cands = _mode_candidates(output_root, mode, symbol)\n        if args.tag:\n            tl = args.tag.lower()\n            cands = [p for p in cands if tl in p.name.lower()]\n        if not cands:\n            raise SystemExit(f"ログが見つかりません: {output_root/\'stepE\'/mode}")\n        for p in cands:\n            print("=" * 90)\n            print(f"[LOG] {p.as_posix()}")\n            results = analyze_one(p, output_root, mode, symbol, args.trade_cost_bps)\n            top = results[0]\n            print(f"Top match: implied=\'{top.implied}\'  return=\'{top.return_def}\'  n={top.n}  mae={top.mae:.6g}  rmse={top.rmse:.6g}  corr={top.corr:.6g}")\n            print("Top 8 candidates:")\n            for r in results[:8]:\n                print(f"  n={r.n:4d}  mae={r.mae:.6g}  rmse={r.rmse:.6g}  corr={r.corr:.6g}  | {r.implied}  vs  {r.return_def}")\n        return 0\n\n    if args.log:\n        log_path = Path(args.log)\n        if not log_path.exists():\n            raise SystemExit(f"--log が見つかりません: {log_path}")\n    else:\n        cands = _mode_candidates(output_root, mode, symbol)\n        if args.tag:\n            tl = args.tag.lower()\n            cands = [p for p in cands if tl in p.name.lower()]\n        if not cands:\n            raise SystemExit(f"ログが見つかりません: {output_root/\'stepE\'/mode}")\n        if args.tag and len(cands) > 1:\n            msg = "\\n".join([p.as_posix() for p in cands[:50]])\n            raise SystemExit(f"複数のログ候補が見つかりました（tag={args.tag}）。--log で明示してください。\\n" + msg)\n        log_path = _pick_default_log(cands)\n        if len(cands) > 1:\n            print("[INFO] 複数候補があるため、最新のログを自動選択しました。別のログを見るには --log を指定するか --all を使ってください。")\n            for p in cands[:10]:\n                mark = "*" if p == log_path else " "\n                print(f"  {mark} {p.as_posix()}")\n\n    print(f"[LOG] {log_path.as_posix()}")\n    results = analyze_one(log_path, output_root, mode, symbol, args.trade_cost_bps)\n    top = results[0]\n    print(f"Top match: implied=\'{top.implied}\'  return=\'{top.return_def}\'  n={top.n}  mae={top.mae:.6g}  rmse={top.rmse:.6g}  corr={top.corr:.6g}")\n    print("Top 12 candidates:")\n    for r in results[:12]:\n        print(f"  n={r.n:4d}  mae={r.mae:.6g}  rmse={r.rmse:.6g}  corr={r.corr:.6g}  | {r.implied}  vs  {r.return_def}")\n    return 0\n\n\nif __name__ == "__main__":\n    raise SystemExit(main())\n'

def main() -> int:
    repo_root = Path(__file__).resolve().parents[1]
    dst = repo_root / 'tools' / 'check_stepE_reward_alignment.py'
    if not dst.exists():
        raise SystemExit(f'Not found: {dst}')

    old_dir = repo_root / 'old'
    old_dir.mkdir(exist_ok=True)

    ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup = old_dir / f'check_stepE_reward_alignment_old_{ts}.py'
    backup.write_text(dst.read_text(encoding='utf-8'), encoding='utf-8')

    dst.write_text(NEW_CONTENT, encoding='utf-8')
    print(f'[fix] UPDATED: {dst}')
    print(f'[fix] backup -> {backup}')
    return 0

if __name__ == '__main__':
    raise SystemExit(main())
